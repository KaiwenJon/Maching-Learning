{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight  Initialization and Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48245,
     "status": "ok",
     "timestamp": 1553457558942,
     "user": {
      "displayName": "張育維",
      "photoUrl": "https://lh3.googleusercontent.com/-s1OKLRh_XmM/AAAAAAAAAAI/AAAAAAAAAAo/OY_rL6C_Unc/s64/photo.jpg",
      "userId": "01333426784061381001"
     },
     "user_tz": -480
    },
    "id": "ynN4MNygRwD-",
    "outputId": "7b2b677b-fb1f-4034-8b23-4afe75a38784"
   },
   "outputs": [],
   "source": [
    "# !nvcc --version\n",
    "!pip install mxnet-cu100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Pyvew5AUrpv"
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, init, nd, autograd\n",
    "from mxnet.gluon import data as gdata, nn, loss as gloss, utils as gutils\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu().\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx\n",
    "\n",
    "def train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,\n",
    "              num_epochs):\n",
    "    \"\"\"Train and evaluate a model with CPU or GPU.\"\"\"\n",
    "    print('training on', ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)            \n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
    "                 time.time() - start))\n",
    "        \n",
    "def evaluate_accuracy(data_iter, net, ctx=[mx.cpu()]):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    acc_sum, n = nd.array([0]), 0\n",
    "    for batch in data_iter:\n",
    "        features, labels, _ = _get_batch(batch, ctx)\n",
    "        for X, y in zip(features, labels):\n",
    "            y = y.astype('float32')\n",
    "            acc_sum += (net(X).argmax(axis=1) == y).sum().copyto(mx.cpu())\n",
    "            n += y.size\n",
    "        acc_sum.wait_to_read()\n",
    "    return acc_sum.asscalar() / n\n",
    "\n",
    "def _get_batch(batch, ctx):\n",
    "    \"\"\"Return features and labels on ctx.\"\"\"\n",
    "    features, labels = batch\n",
    "    if labels.dtype != features.dtype:\n",
    "        labels = labels.astype(features.dtype)\n",
    "    return (gutils.split_and_load(features, ctx),\n",
    "            gutils.split_and_load(labels, ctx), features.shape[0])\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join(\n",
    "        '~', '.mxnet', 'datasets', 'fashion-mnist')):\n",
    "    root = os.path.expanduser(root)  # Expand the user path '~'.\n",
    "    transformer = []\n",
    "    if resize:\n",
    "        transformer += [gdata.vision.transforms.Resize(resize)]\n",
    "    transformer += [gdata.vision.transforms.ToTensor()]\n",
    "    transformer += [gdata.vision.transforms.Normalize(0.13, 0.31)]\n",
    "    transformer = gdata.vision.transforms.Compose(transformer)\n",
    "    mnist_train = gdata.vision.FashionMNIST(root=root, train=True)\n",
    "    mnist_test = gdata.vision.FashionMNIST(root=root, train=False)\n",
    "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "    train_iter = gdata.DataLoader(\n",
    "        mnist_train.transform_first(transformer), batch_size, shuffle=True,\n",
    "        num_workers=num_workers)\n",
    "    test_iter = gdata.DataLoader(\n",
    "        mnist_test.transform_first(transformer), batch_size, shuffle=False,\n",
    "        num_workers=num_workers)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to use different weight initialization method and batch normalization skill to let the 60 layers model can be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "colab": {},
    "colab_type": "code",
    "id": "YOUX8EovRraT"
   },
   "outputs": [],
   "source": [
    "lr, num_epochs, ctx = 1e-3, 20, try_gpu()\n",
    "#############################################################################\n",
    "# TODO: Use batch normalization  skill to let the model can be trained      #                                  \n",
    "#############################################################################\n",
    "net = nn.Sequential()\n",
    "\n",
    "for i in range(60):\n",
    "    net.add(nn.Dense(100, activation=\"tanh\"))    \n",
    "net.add(nn.Dense(10))\n",
    "#############################################################################\n",
    "\n",
    "# https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.initializer.Initializer\n",
    "#############################################################################\n",
    "# TODO: Use different weight initialization method to let the model can be  #\n",
    "# trained                                                                   #\n",
    "#############################################################################\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Normal())\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 946001,
     "status": "ok",
     "timestamp": 1553461269743,
     "user": {
      "displayName": "張育維",
      "photoUrl": "https://lh3.googleusercontent.com/-s1OKLRh_XmM/AAAAAAAAAAI/AAAAAAAAAAo/OY_rL6C_Unc/s64/photo.jpg",
      "userId": "01333426784061381001"
     },
     "user_tz": -480
    },
    "id": "eQ5Fbe5nRrbP",
    "outputId": "9b7ac36c-9039-460c-e31c-d869c96d1ab4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# https://mxnet.incubator.apache.org/versions/0.11.0/api/python/optimization.html#mxnet.optimizer.Optimizer\n",
    "adam_optimizer = mx.optimizer.Adam(learning_rate=lr, beta1=0.8, beta2=0.9)\n",
    "batch_size = 128\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer=adam_optimizer)\n",
    "train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,\n",
    "          num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brfore doning the homework, here are some tips you neeed to know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get the every layer output result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "for i in range(10):\n",
    "    net.add(nn.Dense(10, activation=\"tanh\"))    \n",
    "net.add(nn.Dense(10))\n",
    "net.initialize(force_reinit=True, init=init.Normal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nd.random.uniform(-1,1,(1,10))\n",
    "y = net(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can use net[i] to get the every layer output result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nd.random.uniform(-1,1,(1,10))\n",
    "for i in range(10):\n",
    "    print(x)\n",
    "    x = net[i](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. how to use batchNorm layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you forward the network without using \"with autograd.record():\", the batchNorm layer may out of function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(100, activation=\"tanh\"))\n",
    "net.add(nn.BatchNorm())\n",
    "net.initialize(force_reinit=True, init=init.Normal())\n",
    "\n",
    "x = nd.random.uniform(-1,1,(128,100))\n",
    "y = net(x)\n",
    "\n",
    "print(\"x var:\" + str(np.var(x.asnumpy())))\n",
    "print(\"y var:\" + str(np.var(y.asnumpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(100, activation=\"tanh\"))\n",
    "net.add(nn.BatchNorm())\n",
    "net.initialize(force_reinit=True, init=init.Normal())\n",
    "\n",
    "x = nd.random.uniform(-1,1,(128,100))\n",
    "with autograd.record():\n",
    "    y = net(x)\n",
    "\n",
    "print(\"x var:\" + str(np.var(x.asnumpy())))\n",
    "print(\"y var:\" + str(np.var(y.asnumpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "1. Plot the value distribution picture of output and output gradient in very layer. (In two different weight initialization methods, which are normal and Xavier weight initialization)\n",
    "2. Plot the variance value picture of output and output gradient in very layer. (In two different weight initialization methods, which are normal and Xavier weight initialization)\n",
    "3. Plot the variance value picture of output and output gradient in very layer. (without and with batchNorm in every layer)\n",
    "4. Plot the variance value picture of output in very layer. (In two different weight initialization method, which are normal and Xavier weight initialization. Every layer has batchNorm layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Plot the value distribution picture of output and output gradient in very layer. (In two different weight initialization methods, which are normal and Xavier weight initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mxnet import nd, gluon, init, autograd\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, init, nd, autograd\n",
    "from mxnet.gluon import data as gdata, nn, loss as gloss, utils as gutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(200, activation='tanh', use_bias=False),\n",
    "        nn.Dense(200, activation='tanh', use_bias=False),\n",
    "        nn.Dense(200, activation='tanh', use_bias=False),\n",
    "        nn.Dense(200, activation='tanh', use_bias=False)\n",
    "       )\n",
    "\n",
    "interval = 50\n",
    "x = nd.random.uniform(-1,1,(1,100))\n",
    "net.initialize(force_reinit=True, init=init.Normal())\n",
    "###########################################################################################\n",
    "# TODO: \"\"\"Normal normalization\"\"\"\n",
    "###########################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(200, activation='tanh', use_bias=False),\n",
    "        nn.Dense(200, activation='tanh', use_bias=False),\n",
    "        nn.Dense(200, activation='tanh', use_bias=False),\n",
    "        nn.Dense(200, activation='tanh', use_bias=False)\n",
    "       )\n",
    "\n",
    "interval = 50\n",
    "x = nd.random.uniform(-1,1,(1,100))\n",
    "net.initialize(force_reinit=True, init=init.Xavier())\n",
    "###########################################################################################\n",
    "# TODO: \"\"\"Xavier normalization\"\"\"\n",
    "###########################################################################################  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plot the variance value picture of output and output gradient in very layer. (In two different weight initialization methods, which are normal and Xavier weight initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 60\n",
    "net = nn.Sequential()\n",
    "for i in range(layer_num):\n",
    "    net.add(nn.Dense(100, activation=\"tanh\", use_bias=False))\n",
    "x = nd.random.uniform(-1,1,(1,100))\n",
    "net.initialize(force_reinit=True, init=init.Normal())\n",
    "###########################################################################################\n",
    "# TODO: \"\"\"Normal normalization\"\"\"\n",
    "###########################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 60\n",
    "net = nn.Sequential()\n",
    "for i in range(layer_num):\n",
    "    net.add(nn.Dense(100, activation=\"tanh\", use_bias=False))\n",
    "x = nd.random.uniform(-1,1,(1,100))\n",
    "net.initialize(force_reinit=True, init=init.Xavier())\n",
    "###########################################################################################\n",
    "# TODO: \"\"\"Xavier normalization\"\"\"\n",
    "###########################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "# TODO: \"\"\" Compare two results \"\"\"\n",
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot the variance value picture of output and output gradient in very layer. (In Xavier weight initialization with batchNorm layer and without batchNorm layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 500\n",
    "net = nn.Sequential()\n",
    "for i in range(layer_num):\n",
    "    net.add(nn.Dense(100, activation=\"tanh\", use_bias=False))\n",
    "net.initialize(force_reinit=True, init=init.Xavier())\n",
    "x = nd.random.uniform(-1,1,(32, 100))\n",
    "###########################################################################################\n",
    "# TODO: \"\"\" Without BatchNorm normalization\"\"\"\n",
    "###########################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 500\n",
    "net = nn.Sequential()\n",
    "for i in range(layer_num):\n",
    "    net.add(nn.Dense(100, activation=\"tanh\", use_bias=False))    \n",
    "    net.add(nn.BatchNorm())\n",
    "net.initialize(force_reinit=True, init=init.Xavier())\n",
    "x = nd.random.uniform(-1,1,(32, 100))\n",
    "###########################################################################################\n",
    "# TODO: \"\"\" BatchNorm normalization \"\"\"\n",
    "###########################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "# TODO: \"\"\" Compare two results\"\"\"\n",
    "###########################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot the variance value picture of output in very layer. (In two different weight initialization method, which are normal and Xavier weight initialization. Every layer has batchNorm layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 100\n",
    "net = nn.Sequential()\n",
    "for i in range(layer_num):\n",
    "    net.add(nn.Dense(100, activation=\"tanh\", use_bias=False))    \n",
    "    net.add(nn.BatchNorm())\n",
    "net.initialize(force_reinit=True, init=init.Normal())\n",
    "x = nd.random.uniform(-1,1,(128, 100))\n",
    "###########################################################################################\n",
    "# TODO: \"\"\" Normal initialization \"\"\"\n",
    "###########################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 100\n",
    "net = nn.Sequential()\n",
    "for i in range(layer_num):\n",
    "    net.add(nn.Dense(100, activation=\"tanh\", use_bias=False))    \n",
    "    net.add(nn.BatchNorm())\n",
    "net.initialize(force_reinit=True, init=init.Xavier())\n",
    "x = nd.random.uniform(-1,1,(32, 100))\n",
    "###########################################################################################\n",
    "# TODO: \"\"\" Xavier initialization \"\"\"\n",
    "###########################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "alexnet.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
